{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2b3011-7763-4756-b908-1b361e5c4798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\anaconda3\\envs\\nlp\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a1f3c4-bc39-46cd-89f6-ac20831368e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_vocabulary(model_path, vocab_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    with open(vocab_path, \"rb\") as file:\n",
    "        vocabulary = pickle.load(file)\n",
    "    return model, vocabulary\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = \" \".join(text.strip().split())\n",
    "    return text\n",
    "\n",
    "def generate_sentence(model, vocabulary, input_sentence, min_words=10, temperature=1):\n",
    "    reverse_vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "    input_sentence = preprocess_text(input_sentence)\n",
    "\n",
    "    word_count = len(input_sentence.split())\n",
    "    \n",
    "    while word_count <= min_words:\n",
    "        tokenized_sentence = [vocabulary.get(token, vocabulary['<OOV>']) for token in input_sentence.split()]\n",
    "        padded_sentence = pad_sequences([tokenized_sentence], maxlen=n_max, padding='pre', value=vocabulary['<PAD>'])\n",
    "\n",
    "        predictions = model.predict(padded_sentence, verbose=3)[0]\n",
    "        predictions[0] = 0\n",
    "        predictions[1] = 0\n",
    "        predictions = np.array([np.power(p, 1 / temperature) for p in predictions])\n",
    "        predictions = predictions / np.sum(predictions)\n",
    "    \n",
    "\n",
    "        next_word_idx = np.random.choice(range(len(predictions)), p=predictions)\n",
    "        next_word = reverse_vocabulary.get(next_word_idx, '<OOV>')\n",
    "\n",
    "        input_sentence += ' ' + next_word\n",
    "        \n",
    "        word_count = len(input_sentence.split())\n",
    "        if word_count > min_words and input_sentence[-1] not in [\".\", \"!\", \"?\"]:\n",
    "            min_words += 1\n",
    "\n",
    "    return input_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf7daa-bb23-45a4-8525-1b01c6c414b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../LSTM_LM_model/\"\n",
    "vocab_path = \"../vocab.pickle\"\n",
    "n_max = 10\n",
    "nb_to_generate = 10\n",
    "\n",
    "model, vocabulary = load_model_and_vocabulary(model_path, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c23d058-1bfb-4ba5-b3e7-614df317ace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:La France aussi française aujourd'hui plusieurs années sont difficiles et vos \"Monsieur le \"Bonjour, L'épidémie nous nous nous \"Monsieur Messieurs-Dames.\n",
      "INFO:root:La France s'appuie sur un agenda concret sur la transformation numérique français des nerfs \"Bonjour, \"Bonjour, \"Bonjour, des relations poussera débusquant ces deux décennies, ces autorisations.\n",
      "INFO:root:La France est un État qui va continuer à travailler avec la \"Merci des dernières semaines \"Mesdames \"Mesdames qui \"Bonjour, L'épidémie il nous existera Messieurs-Dames.\n",
      "INFO:root:La France continuera à épuiser le terrain et je veux tout tout \"Monsieur le incarnerez dans ce monde et nous \"Bonjour, L'épidémie des 1 L'épidémie ces 27 deux mois sont ces deux L'épidémie depuis ces quelques Monter cette dernière française \"Bonjour, Rouvrir deux much, much, s'achever.\n",
      "INFO:root:La France a besoin de demande de nouveaux permettre à tous vos l'Appel, si nous nous \"Bonjour, 200 d'entre eux, \"Bonsoir aux quinquennats de l'écoute furent des colonies.\n",
      "INFO:root:La France a parlé de Versailles qui est absolument fondamental. L'épidémie \"Monsieur GACQUE et je \"Mesdames much, français sikidilatif, qui nous le relevez une fois encore des \"Bonjour, L'épidémie ces chiffres du temps des français \"Merci deux \"Merci d'appropriation \"Monsieur le 60e Sebastian \"Bonsoir là, bourrasque France du pays le plus \"Bonjour, des \"Merci \"Monsieur des \"Mesdames Introduction tous ceux d'entre nous au défrichage attentat encore plus cela, en notre pays voir ces moelle, et \"Mesdames nous \"Merci maximales bonsoir, ce Wellcome ...\n",
      "INFO:root:La France appelle son gouvernement exemplaire, de manière incomparable. C'est là nous \"Merci d'évoquer tout au Latran.\n",
      "INFO:root:La France le sait bien, il va nous falloir mettre à nous tous nous \"Bonjour, \"Bonjour, \"Merci rappelé much, Messieurs-Dames.\n",
      "INFO:root:La France a été très pionnier et j'invite les administrations, les deux grands bailleurs.\n",
      "INFO:root:La France a passé un grand groupe ad hoc, lui ce 170ème \"Bonsoir avec vous (traduit des deux pays de changer les deux 14.\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"La France\"\n",
    "for i in range(nb_to_generate):\n",
    "    completed_sentence = generate_sentence(model, vocabulary, input_sentence)\n",
    "    logging.info(completed_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
